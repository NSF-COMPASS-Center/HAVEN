{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd64c278",
   "metadata": {},
   "source": [
    "## Interpretation of Hybrid VirProBERT attention values for multiclass classification\n",
    "\n",
    "### Trainining Dataset: UNiRef90  - Coronaviridae Spike protein sequences aligned using MAFFT\n",
    "### Interpretation: SARS-CoV-2 Spike protein sequences\n",
    "\n",
    "**Positional Embedding**: Sin-Cos\n",
    "\n",
    "**Maximum Sequence Length**: -\n",
    "\n",
    "**Classification**: Multi-class\n",
    "\n",
    "**\\# classes**: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f0a7a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/interpretability/coronaviridae-aligned',\n",
       " '/opt/conda/lib/python38.zip',\n",
       " '/opt/conda/lib/python3.8',\n",
       " '/opt/conda/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/home/blessyantony/.local/lib/python3.8/site-packages',\n",
       " '/opt/conda/lib/python3.8/site-packages',\n",
       " '/opt/conda/lib/python3.8/site-packages/IPython/extensions',\n",
       " '/home/blessyantony/.ipython',\n",
       " '/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/interpretability/coronaviridae-aligned/../../..',\n",
       " '/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/interpretability/coronaviridae-aligned/../../../..',\n",
       " '/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/interpretability/coronaviridae-aligned/../..',\n",
       " '/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/interpretability/coronaviridae-aligned/..']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\", \"..\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"..\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "256b5134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.nlp.transformer import transformer\n",
    "from models.nlp.hybrid import transformer_attention\n",
    "from datasets.protein_sequence_dataset import ProteinSequenceDataset\n",
    "from src.utils import utils, dataset_utils, nn_utils, constants\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "cmap = sns.color_palette(\"vlag\", as_cmap=True)\n",
    "\n",
    "from sklearn.metrics import roc_curve, accuracy_score, f1_score, auc, precision_recall_curve\n",
    "from statistics import mean\n",
    "\n",
    "# from captum.attr import LayerIntegratedGradients, TokenReferenceBase, LayerGradientXActivation, LayerDeepLift, LayerLRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0c3063",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_groupings = {\n",
    "                    \"Chicken\": [ \"gallus gallus\" ],\n",
    "                    \"Human\": [ \"homo sapiens\" ],\n",
    "                    \"Cat\": [ \"felis catus\" ],\n",
    "                    \"Pig\": [ \"sus scrofa\" ],\n",
    "                    \"Gray wolf\": [ \"canis lupus\" ],\n",
    "                    \"Horshoe bat\": [\"rhinolophus sp.\"],\n",
    "                    \"Ferret\": [\"mustela putorius\"],\n",
    "                    \"Chinese rufous horseshoe bat\": [\"rhinolophus sinicus\"]\n",
    "                }\n",
    "\n",
    "\n",
    "sequence_settings =  {\n",
    "    \"sequence_col\": \"aligned_seq\",\n",
    "    \"batch_size\": 16,\n",
    "    \"max_sequence_length\": 128,\n",
    "    \"truncate\": False,\n",
    "    \"split_sequence\": False,\n",
    "    \"feature_type\": \"token\",\n",
    "}\n",
    "\n",
    "label_settings = {\n",
    "    \"label_col\": \"virus_host_name\",\n",
    "    \"exclude_labels\": [ \"nan\"],\n",
    "    \"label_groupings\":  label_groupings\n",
    "}\n",
    "\n",
    "model = {\n",
    "    \"pre_train_settings\": {\n",
    "        \"n_heads\": 8,\n",
    "        \"depth\": 6,\n",
    "        \"input_dim\": 512, # input embedding dimension\n",
    "        \"hidden_dim\": 1024,\n",
    "        \"max_seq_len\": 129,\n",
    "    },\n",
    "    \"loss\": \"FocalLoss\",\n",
    "    \"n_heads\": 8,\n",
    "    \"depth\": 2,\n",
    "    \"stride\": 64,\n",
    "    \"n_classes\": 8,\n",
    "    \"input_dim\": 512, # input embedding dimension\n",
    "    \"hidden_dim\": 1024,\n",
    "    \"cls_token\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a4b303",
   "metadata": {},
   "source": [
    "### Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dc7319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_loader(dataset_loader):\n",
    "    print()\n",
    "    sequence, label = next(iter(dataset_loader))\n",
    "    print(f\"Sequence tensor size = {sequence.shape}\")\n",
    "    print(f\"Sequence = {sequence}\")\n",
    "    print(f\"Label tensor size = {label.shape}\")\n",
    "    print(f\"Label = {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90e657b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniref90_id</th>\n",
       "      <th>aligned_seq</th>\n",
       "      <th>seq</th>\n",
       "      <th>virus_name</th>\n",
       "      <th>virus_host_name</th>\n",
       "      <th>human_binary_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WIV04</td>\n",
       "      <td>--------------MFVFLVLLPLVSS--------Q----------...</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>WIV04(MN996528.1) Wuhan variant index virus</td>\n",
       "      <td>homo sapiens</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UniRef90_A0A7U3RIT3</td>\n",
       "      <td>--------------MFVFLVLVPLVSS--------Q----------...</td>\n",
       "      <td>MFVFLVLVPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>homo sapiens</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UniRef90_A0A7U3HGG2</td>\n",
       "      <td>--------------MFVFLVLLPLVSS--------Q----------...</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>homo sapiens</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UniRef90_A0A7U3EEN6</td>\n",
       "      <td>--------------MFVFLVLLPLVSS--------Q----------...</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>homo sapiens</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UniRef90_A0A7U3HDM5</td>\n",
       "      <td>--------------MFVFLVLLPLVSS--------Q----------...</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>homo sapiens</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>UniRef90_S5FZ76</td>\n",
       "      <td>---------------------------TLKQ---------------...</td>\n",
       "      <td>TLKQCDASAGYYSSSPIRPSDGVHSVTGFYRPVKTCCIKYTYPSNT...</td>\n",
       "      <td>Infectious bronchitis virus</td>\n",
       "      <td>gallus gallus</td>\n",
       "      <td>NOT homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>UniRef90_U5WLM9</td>\n",
       "      <td>--------------MLLLVTLFGLASG-------------------...</td>\n",
       "      <td>MLLLVTLFGLASGCSLPLTVSCPRGLPFTLQINTTSVTVEWYRVSP...</td>\n",
       "      <td>Sarbecovirus</td>\n",
       "      <td>rhinolophus sinicus</td>\n",
       "      <td>NOT homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>UniRef90_A0A169QA14</td>\n",
       "      <td>--MILHF-IMKVMPILIMVVFILL----------------------...</td>\n",
       "      <td>MILHFIMKVMPILIMVVFILLVYTNTHSSEWLLLFYFLISGVFCLY...</td>\n",
       "      <td>Infectious bronchitis virus</td>\n",
       "      <td>gallus gallus</td>\n",
       "      <td>NOT homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>UniRef90_E7DBM7</td>\n",
       "      <td>----------------------------------------------...</td>\n",
       "      <td>CSRRQFENYNQIEKVHVH</td>\n",
       "      <td>Feline coronavirus</td>\n",
       "      <td>felis catus</td>\n",
       "      <td>NOT homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>UniRef90_U3U789</td>\n",
       "      <td>----------------------------------------------...</td>\n",
       "      <td>LIVKDVSLTLFKNHDGKLYLTPRTTYEPRVAT</td>\n",
       "      <td>Ferret coronavirus</td>\n",
       "      <td>mustela putorius</td>\n",
       "      <td>NOT homo sapiens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>682 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             uniref90_id                                        aligned_seq  \\\n",
       "0                  WIV04  --------------MFVFLVLLPLVSS--------Q----------...   \n",
       "1    UniRef90_A0A7U3RIT3  --------------MFVFLVLVPLVSS--------Q----------...   \n",
       "2    UniRef90_A0A7U3HGG2  --------------MFVFLVLLPLVSS--------Q----------...   \n",
       "3    UniRef90_A0A7U3EEN6  --------------MFVFLVLLPLVSS--------Q----------...   \n",
       "4    UniRef90_A0A7U3HDM5  --------------MFVFLVLLPLVSS--------Q----------...   \n",
       "..                   ...                                                ...   \n",
       "677      UniRef90_S5FZ76  ---------------------------TLKQ---------------...   \n",
       "678      UniRef90_U5WLM9  --------------MLLLVTLFGLASG-------------------...   \n",
       "679  UniRef90_A0A169QA14  --MILHF-IMKVMPILIMVVFILL----------------------...   \n",
       "680      UniRef90_E7DBM7  ----------------------------------------------...   \n",
       "681      UniRef90_U3U789  ----------------------------------------------...   \n",
       "\n",
       "                                                   seq  \\\n",
       "0    MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...   \n",
       "1    MFVFLVLVPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...   \n",
       "2    MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...   \n",
       "3    MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...   \n",
       "4    MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...   \n",
       "..                                                 ...   \n",
       "677  TLKQCDASAGYYSSSPIRPSDGVHSVTGFYRPVKTCCIKYTYPSNT...   \n",
       "678  MLLLVTLFGLASGCSLPLTVSCPRGLPFTLQINTTSVTVEWYRVSP...   \n",
       "679  MILHFIMKVMPILIMVVFILLVYTNTHSSEWLLLFYFLISGVFCLY...   \n",
       "680                                 CSRRQFENYNQIEKVHVH   \n",
       "681                   LIVKDVSLTLFKNHDGKLYLTPRTTYEPRVAT   \n",
       "\n",
       "                                          virus_name      virus_host_name  \\\n",
       "0        WIV04(MN996528.1) Wuhan variant index virus         homo sapiens   \n",
       "1    Severe acute respiratory syndrome coronavirus 2         homo sapiens   \n",
       "2    Severe acute respiratory syndrome coronavirus 2         homo sapiens   \n",
       "3    Severe acute respiratory syndrome coronavirus 2         homo sapiens   \n",
       "4    Severe acute respiratory syndrome coronavirus 2         homo sapiens   \n",
       "..                                               ...                  ...   \n",
       "677                      Infectious bronchitis virus        gallus gallus   \n",
       "678                                     Sarbecovirus  rhinolophus sinicus   \n",
       "679                      Infectious bronchitis virus        gallus gallus   \n",
       "680                               Feline coronavirus          felis catus   \n",
       "681                               Ferret coronavirus     mustela putorius   \n",
       "\n",
       "    human_binary_label  \n",
       "0         homo sapiens  \n",
       "1         homo sapiens  \n",
       "2         homo sapiens  \n",
       "3         homo sapiens  \n",
       "4         homo sapiens  \n",
       "..                 ...  \n",
       "677   NOT homo sapiens  \n",
       "678   NOT homo sapiens  \n",
       "679   NOT homo sapiens  \n",
       "680   NOT homo sapiens  \n",
       "681   NOT homo sapiens  \n",
       "\n",
       "[682 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file_path = os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"..\", \"input/data/coronaviridae/20240313/uniref/alignment/coronaviridae_s_uniref90_embl_hosts_pruned_metadata_corrected_species_virus_host_vertebrates_w_seq_t0.01_c8_aligned.csv\")\n",
    "uniref90_coronaviridae_aligned_df = pd.read_csv(input_file_path)\n",
    "wiv04_seq_df = uniref90_coronaviridae_aligned_df[uniref90_coronaviridae_aligned_df[\"uniref90_id\"] == \"WIV04\"]\n",
    "uniref90_coronaviridae_aligned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c88a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping labels using config : {'Chicken': ['gallus gallus'], 'Human': ['homo sapiens'], 'Cat': ['felis catus'], 'Pig': ['sus scrofa'], 'Gray wolf': ['canis lupus'], 'Horshoe bat': ['rhinolophus sp.'], 'Ferret': ['mustela putorius'], 'Chinese rufous horseshoe bat': ['rhinolophus sinicus']}\n",
      "label_idx_map={'Cat': 0, 'Chicken': 1, 'Chinese rufous horseshoe bat': 2, 'Ferret': 3, 'Gray wolf': 4, 'Horshoe bat': 5, 'Human': 6, 'Pig': 7}\n",
      "idx_label_map={0: 'Cat', 1: 'Chicken', 2: 'Chinese rufous horseshoe bat', 3: 'Ferret', 4: 'Gray wolf', 5: 'Horshoe bat', 6: 'Human', 7: 'Pig'}\n",
      "\n",
      "Sequence tensor size = torch.Size([16, 2418])\n",
      "Sequence = tensor([[27., 27., 27.,  ..., 27., 27., 27.],\n",
      "        [27., 27., 27.,  ..., 27., 27., 27.],\n",
      "        [27., 27., 27.,  ..., 19., 27., 27.],\n",
      "        ...,\n",
      "        [27., 27., 27.,  ..., 27., 27., 27.],\n",
      "        [27., 27., 27.,  ..., 27., 27., 27.],\n",
      "        [27., 27., 27.,  ..., 19., 27., 27.]], dtype=torch.float64)\n",
      "Label tensor size = torch.Size([16])\n",
      "Label = tensor([1, 1, 6, 1, 1, 1, 6, 1, 6, 1, 2, 6, 1, 4, 0, 6])\n"
     ]
    }
   ],
   "source": [
    "index_label_map, dataset_loader = dataset_utils.load_dataset_with_df(uniref90_coronaviridae_aligned_df, sequence_settings, label_settings, label_col=label_settings[\"label_col\"], classification_type=\"multi\")\n",
    "print_dataset_loader(dataset_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab719dd1",
   "metadata": {},
   "source": [
    "### Load the pre-trained and fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18bd3a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerEncoder(\n",
      "  (embedding): EmbeddingLayer(\n",
      "    (token_embedding): Embedding(30, 512, padding_idx=0)\n",
      "    (positional_embedding): PositionalEncoding()\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): NormalizationLayer()\n",
      "  )\n",
      ")\n",
      "Number of parameters =  12619776\n"
     ]
    }
   ],
   "source": [
    "pre_train_encoder_settings = model[\"pre_train_settings\"]\n",
    "pre_train_encoder_settings[\"vocab_size\"] = 30 #constants.VOCAB_SIZE\n",
    "pre_trained_encoder_model = transformer.get_transformer_encoder(pre_train_encoder_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0031a314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerAttention(\n",
      "  (pre_trained_model): TransformerEncoder(\n",
      "    (embedding): EmbeddingLayer(\n",
      "      (token_embedding): Embedding(30, 512, padding_idx=0)\n",
      "      (positional_embedding): PositionalEncoding()\n",
      "    )\n",
      "    (encoder): Encoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm): NormalizationLayer()\n",
      "    )\n",
      "  )\n",
      "  (self_attn): MultiHeadAttention(\n",
      "    (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (feed_forward): FeedForwardLayer(\n",
      "    (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  )\n",
      "  (linear_ip): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (linear_hidden): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (linear_hidden_n): ModuleList(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear_op): Linear(in_features=1024, out_features=8, bias=True)\n",
      ")\n",
      "Number of parameters =  18402824\n"
     ]
    }
   ],
   "source": [
    "model[\"pre_trained_model\"] = pre_trained_encoder_model\n",
    "model[\"segment_len\"] = sequence_settings[\"max_sequence_length\"]\n",
    "prediction_model = transformer_attention.get_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fe14fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"..\", \"output/raw/coronaviridae_s_prot_uniref90_embl_vertebrates_aligned_t0.01_c8/20240711/host_multi/fine_tuning_hybrid_cls_vs30/mlm_tfenc_l6_h8_lr1e-4_uniref90viridae_msl128b1024vs30_hybrid_attention_s64_fnn_2l_d1024_lr1e-4_itr4.pth\")\n",
    "prediction_model.load_state_dict(torch.load(model_path, map_location=nn_utils.get_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e77de3",
   "metadata": {},
   "source": [
    "### t-SNE Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15f6a14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(model, dataset_loader):\n",
    "    model.eval()\n",
    "    seq_dfs = []\n",
    "    for _, record in enumerate(dataset_loader):\n",
    "        seq, label = record\n",
    "        output = model(seq)\n",
    "        # embedding = value for each dimension = mean of the dimensional values of all tokens in the input sequence\n",
    "        seq_encoding = model.embedding\n",
    "        seq_df = pd.DataFrame(seq_encoding.squeeze().cpu().detach().numpy())\n",
    "        seq_df[\"label\"] = label.squeeze().cpu().detach().numpy()\n",
    "        print(seq_df.shape)\n",
    "        seq_dfs.append(seq_df)\n",
    "    df = pd.concat(seq_dfs)\n",
    "    print(df.shape)\n",
    "    return df\n",
    "\n",
    "def view_tsne_representation(rep_df, index_label_map):\n",
    "    columns = rep_df.columns\n",
    "    print(columns)\n",
    "    X = rep_df[range(512)]\n",
    "    tsne_model = TSNE(n_components=2, verbose=1, init=\"pca\", learning_rate=\"auto\").fit(X)\n",
    "    X_tsne_emb = pd.DataFrame(tsne_model.fit_transform(X))\n",
    "    print(X_tsne_emb.shape)\n",
    "    print(X_tsne_emb)\n",
    "    X_tsne_emb[\"label\"] = rep_df[\"label\"].values\n",
    "    X_tsne_emb[\"label\"] = X_tsne_emb[\"label\"].map(index_label_map)\n",
    "    \n",
    "    sns.scatterplot(data = X_tsne_emb, x=0, y=1, hue=\"label\")\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.show()\n",
    "    return tsne_model, X_tsne_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cba83ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emb_df = compute_embeddings(prediction_model, dataset_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e854b6",
   "metadata": {},
   "source": [
    "### Attention value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2032bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attention_of_sequence(model, seq):\n",
    "    print(f\"sequence length = {seq_len}\")\n",
    "    model.eval()\n",
    "    output = model(seq)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "406b856c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniref90_id</th>\n",
       "      <th>aligned_seq</th>\n",
       "      <th>seq</th>\n",
       "      <th>virus_name</th>\n",
       "      <th>virus_host_name</th>\n",
       "      <th>human_binary_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WIV04</td>\n",
       "      <td>--------------MFVFLVLLPLVSS--------Q----------...</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>WIV04(MN996528.1) Wuhan variant index virus</td>\n",
       "      <td>homo sapiens</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  uniref90_id                                        aligned_seq  \\\n",
       "0       WIV04  --------------MFVFLVLLPLVSS--------Q----------...   \n",
       "\n",
       "                                                 seq  \\\n",
       "0  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...   \n",
       "\n",
       "                                    virus_name virus_host_name  \\\n",
       "0  WIV04(MN996528.1) Wuhan variant index virus    homo sapiens   \n",
       "\n",
       "  human_binary_label  \n",
       "0       homo sapiens  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiv04_seq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c58128e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2418"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiv04_seq_df[\"aligned_seq\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5489d0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping labels using config : {'Chicken': ['gallus gallus'], 'Human': ['homo sapiens'], 'Cat': ['felis catus'], 'Pig': ['sus scrofa'], 'Gray wolf': ['canis lupus'], 'Horshoe bat': ['rhinolophus sp.'], 'Ferret': ['mustela putorius'], 'Chinese rufous horseshoe bat': ['rhinolophus sinicus']}\n",
      "label_idx_map={'Cat': 0, 'Chicken': 1, 'Chinese rufous horseshoe bat': 2, 'Ferret': 3, 'Gray wolf': 4, 'Horshoe bat': 5, 'Human': 6, 'Pig': 7}\n",
      "idx_label_map={0: 'Cat', 1: 'Chicken', 2: 'Chinese rufous horseshoe bat', 3: 'Ferret', 4: 'Gray wolf', 5: 'Horshoe bat', 6: 'Human', 7: 'Pig'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "/opt/conda/lib/python3.8/site-packages/pandas/core/indexing.py:723: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n",
      "/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/interpretability/coronaviridae-aligned/../../../utils/utils.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[label_col] = df[label_col].transform(lambda x: label_idx_map[x] if x in label_idx_map else 0)\n"
     ]
    }
   ],
   "source": [
    "sequence_settings[\"batch_size\"] = 1\n",
    "sequence_settings[\"max_sequence_length\"] = 128\n",
    "\n",
    "_, wiv04_seq_df_dataset_loader = dataset_utils.load_dataset_with_df(wiv04_seq_df, sequence_settings, label_settings, label_col=label_settings[\"label_col\"], classification_type=\"multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d2d2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model.eval()\n",
    "input, label = next(iter(wiv04_seq_df_dataset_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ef34d58",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3841814/315635475.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/git/zoonosis/src/jupyter_notebooks/interpretability/coronaviridae-aligned/../../../models/nlp/hybrid/transformer_attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# generate embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_trained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (b * n_s) x segment_len x input_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# reshape back into sequences of chunks, i.e. re-introduce the batch dimension.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/git/zoonosis/src/jupyter_notebooks/interpretability/coronaviridae-aligned/../../../models/nlp/transformer/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, mask)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/git/zoonosis/src/jupyter_notebooks/interpretability/coronaviridae-aligned/../../../models/nlp/embedding/embedding.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2141\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2142\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2143\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "output = prediction_model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7479b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa2285",
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af274ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464f3c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model.pre_trained_model.encoder.layers[-1].self_attn.self_attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a490a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model.self_attn.self_attn.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf86417",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiv04_position_mapping = {}\n",
    "pos = 0\n",
    "for i, token in enumerate(wiv04_seq_df[\"aligned_seq\"][0]):\n",
    "    if token == \"-\":\n",
    "        pass\n",
    "    else:\n",
    "        pos += 1\n",
    "        wiv04_position_mapping[i] = int(pos)\n",
    "    if (i%64 == 0) and (i not in wiv04_position_mapping):\n",
    "        wiv04_position_mapping[i] = pos        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f0ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wiv04_position_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4700967",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiv04_position_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dec741",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_mapping = {}\n",
    "j = 0\n",
    "for i in range(0, len(wiv04_seq_df[\"aligned_seq\"][0]), 64):\n",
    "    try:\n",
    "        pos_mapping[j] = f\"{j}: {wiv04_position_mapping[i]}-{wiv04_position_mapping[i+128]}\"\n",
    "    except KeyError:\n",
    "        break\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9063a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aee9ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_seg_attn = prediction_model.self_attn.self_attn.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb97f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.rcParams[\"xtick.labelsize\"] = 40\n",
    "plt.rcParams[\"ytick.labelsize\"] = 40\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "fig, axs = plt.subplots(4, 2, figsize=(80, 100), sharex=False, sharey=True)\n",
    "\n",
    "c = 0\n",
    "for i in range(4):\n",
    "    for j in range(2):\n",
    "        df = pd.DataFrame(inter_seg_attn[c].squeeze().detach().cpu().numpy())\n",
    "        df.rename(columns=pos_mapping, inplace=True)\n",
    "        df.rename(index=pos_mapping, inplace=True)\n",
    "        sns.heatmap(df, ax=axs[i, j], linewidth=.1)\n",
    "        axs[i, j].set_title(f\"Head {c}\")\n",
    "        c += 1\n",
    "\n",
    "plt.tight_layout(pad=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad22c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_seg_index = 20\n",
    "intra_seg_attn = prediction_model.pre_trained_model.encoder.layers[-1].self_attn.self_attn[intra_seg_index].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a03009",
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_seg_attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f191f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_seg_pos_map = {}\n",
    "intra_seg_start = intra_seg_index * 64\n",
    "intra_seg_end = intra_seg_index * 64 + 128\n",
    "\n",
    "c = 0\n",
    "for i in range(intra_seg_start, intra_seg_end + 1):\n",
    "    if i in wiv04_position_mapping:\n",
    "        intra_seg_pos_map[c] = wiv04_position_mapping[i]\n",
    "    else:\n",
    "        intra_seg_pos_map[c] = \"-\"\n",
    "    c += 1\n",
    "\n",
    "intra_seg_pos_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d18c5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.rcParams[\"xtick.labelsize\"] = 40\n",
    "plt.rcParams[\"ytick.labelsize\"] = 40\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "fig, axs = plt.subplots(4, 2, figsize=(80, 100), sharex=False, sharey=False)\n",
    "\n",
    "c = 0\n",
    "for i in range(4):\n",
    "    for j in range(2):\n",
    "        df = pd.DataFrame(intra_seg_attn[c].squeeze().detach().cpu().numpy())\n",
    "        df.rename(columns=intra_seg_pos_map, inplace=True)\n",
    "        df.rename(index=intra_seg_pos_map, inplace=True)\n",
    "        sns.heatmap(df, ax=axs[i, j], linewidth=.1)\n",
    "        axs[i, j].set_title(f\"Head {c}\")\n",
    "        c += 1\n",
    "\n",
    "plt.tight_layout(pad=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a966bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.rcParams[\"xtick.labelsize\"] = 20\n",
    "plt.rcParams[\"ytick.labelsize\"] = 20\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "fig, axs = plt.subplots(1, 1, figsize=(30, 30), sharex=False, sharey=False)\n",
    "\n",
    "df = pd.DataFrame(intra_seg_attn.mean(dim=0).detach().cpu().numpy())\n",
    "df.rename(columns=intra_seg_pos_map, inplace=True)\n",
    "df.rename(index=intra_seg_pos_map, inplace=True)\n",
    "sns.heatmap(df, ax=axs, linewidth=.1)\n",
    "\n",
    "plt.tight_layout(pad=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f7137",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_dash_indices = [k for k, v in intra_seg_pos_map.items() if v != \"-\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a927b198",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.rcParams[\"xtick.labelsize\"] = 30\n",
    "plt.rcParams[\"ytick.labelsize\"] = 30\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "fig, axs = plt.subplots(1, 1, figsize=(40, 40), sharex=False, sharey=False)\n",
    "\n",
    "df = pd.DataFrame(intra_seg_attn.mean(dim=0).detach().cpu().numpy())\n",
    "df = df.iloc[non_dash_indices]\n",
    "df.rename(columns=intra_seg_pos_map, inplace=True)\n",
    "df.rename(index=intra_seg_pos_map, inplace=True)\n",
    "sns.heatmap(df, ax=axs, linewidth=.1)\n",
    "\n",
    "plt.tight_layout(pad=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be57abbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
