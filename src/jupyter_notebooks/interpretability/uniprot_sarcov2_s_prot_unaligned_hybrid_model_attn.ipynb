{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd64c278",
   "metadata": {},
   "source": [
    "## Interpretation of Hybrid VirProBERT attention values for multiclass classification\n",
    "\n",
    "### Trainining Dataset: UniProt  - Coronaviridae Spike protein sequences (unaligned)\n",
    "### Interpretation: SARS-CoV-2 Spike protein sequences\n",
    "\n",
    "**Positional Embedding**: Sin-Cos\n",
    "\n",
    "**Maximum Sequence Length**: -\n",
    "\n",
    "**Classification**: Multi-class\n",
    "\n",
    "**\\# classes**: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f0a7a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/interpretability',\n",
       " '/opt/conda/lib/python38.zip',\n",
       " '/opt/conda/lib/python3.8',\n",
       " '/opt/conda/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/home/blessyantony/.local/lib/python3.8/site-packages',\n",
       " '/opt/conda/lib/python3.8/site-packages',\n",
       " '/opt/conda/lib/python3.8/site-packages/IPython/extensions',\n",
       " '/home/blessyantony/.ipython',\n",
       " '/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/interpretability/../../..',\n",
       " '/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/interpretability/../../../..',\n",
       " '/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/interpretability/../..',\n",
       " '/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/interpretability/..']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\", \"..\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"..\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "256b5134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.nlp.transformer import transformer\n",
    "from models.nlp.hybrid import transformer_attention\n",
    "from datasets.protein_sequence_dataset import ProteinSequenceDataset\n",
    "from src.utils import utils, dataset_utils, nn_utils, constants\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "cmap = sns.color_palette(\"vlag\", as_cmap=True)\n",
    "\n",
    "from sklearn.metrics import roc_curve, accuracy_score, f1_score, auc, precision_recall_curve\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0c3063",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_groupings = {\n",
    "                    \"Pig\": [ \"sus scrofa\" ],\n",
    "                    \"Human\": [ \"homo sapiens\" ],\n",
    "                    \"Cat\": [ \"felis catus\" ],\n",
    "                    \"Dromedary camel\": [ \"camelus dromedarius\" ],\n",
    "                    \"Cattle\": [\"bos taurus\"],\n",
    "                    \"Chicken\": [ \"gallus gallus\" ],\n",
    "                    \"Gray wolf\": [ \"canis lupus\" ],\n",
    "                    \"Yak\": [ \"bos grunniens\" ]\n",
    "                }\n",
    "\n",
    "\n",
    "sequence_settings =  {\n",
    "    \"sequence_col\": \"seq\",\n",
    "    \"batch_size\": 1,\n",
    "    \"max_sequence_length\": 256,\n",
    "    \"truncate\": False,\n",
    "    \"split_sequence\": False,\n",
    "    \"feature_type\": \"token\",\n",
    "}\n",
    "\n",
    "label_settings = {\n",
    "    \"label_col\": \"virus_host_name\",\n",
    "    \"exclude_labels\": [ \"nan\"],\n",
    "    \"label_groupings\":  label_groupings\n",
    "}\n",
    "\n",
    "model = {\n",
    "    \"pre_train_settings\": {\n",
    "        \"n_heads\": 8,\n",
    "        \"depth\": 6,\n",
    "        \"input_dim\": 512, # input embedding dimension\n",
    "        \"hidden_dim\": 1024,\n",
    "        \"max_seq_len\": 256,\n",
    "    },\n",
    "    \"loss\": \"FocalLoss\",\n",
    "    \"n_heads\": 8,\n",
    "    \"depth\": 2,\n",
    "    \"stride\": 64,\n",
    "    \"n_classes\": 8,\n",
    "    \"input_dim\": 512, # input embedding dimension\n",
    "    \"hidden_dim\": 1024,\n",
    "    \"cls_token\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a4b303",
   "metadata": {},
   "source": [
    "### Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dc7319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_loader(dataset_loader):\n",
    "    print()\n",
    "    sequence, label = next(iter(dataset_loader))\n",
    "    print(f\"Sequence tensor size = {sequence.shape}\")\n",
    "    print(f\"Sequence = {sequence}\")\n",
    "    print(f\"Label tensor size = {label.shape}\")\n",
    "    print(f\"Label = {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90e657b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>seq</th>\n",
       "      <th>virus_host_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WIV04</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QQM19141.1</td>\n",
       "      <td>MFVFLVLLPLVSIQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QQX30509.1</td>\n",
       "      <td>VNXXTRTQLXXXXXXXXTRGVYYPDKVFRSSVLHSTQDLFLPFFSN...</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QRF70806.1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QRN78347.1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>QRX39425.1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNFTNRTQLPSAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>QRX49325.1</td>\n",
       "      <td>MFVFFVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>QTJ90974.1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>QTY83052.1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>QUD52764.1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLRTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>QWE88920.1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>UFO69279.1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>UOZ45804.1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLITRTQSYTNSFTRGVYYPDKVFRSSVLH...</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>UPP14409.1</td>\n",
       "      <td>MFGFLVLLPLVSSQCVNLITRTQSYTNSFTRGVYYPDKVFRSSVLH...</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>UWM38596.1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLITRTQSYTNSFTRGVYYPDKVFRSSVLH...</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>UZG29433.1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLITRTQSYTNSFTRGVYYPDKVFRSSVLH...</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>WGM84363.1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLITRTQSYTNSFTRGVYYPDKVFRSSVLH...</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>WRK13149.1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLITTTQSYTNSFTRGVYYPDKVFRSSVLH...</td>\n",
       "      <td>homo sapiens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                                seq  \\\n",
       "0        WIV04  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...   \n",
       "1   QQM19141.1  MFVFLVLLPLVSIQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...   \n",
       "2   QQX30509.1  VNXXTRTQLXXXXXXXXTRGVYYPDKVFRSSVLHSTQDLFLPFFSN...   \n",
       "3   QRF70806.1  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...   \n",
       "4   QRN78347.1  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...   \n",
       "5   QRX39425.1  MFVFLVLLPLVSSQCVNFTNRTQLPSAYTNSFTRGVYYPDKVFRSS...   \n",
       "6   QRX49325.1  MFVFFVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...   \n",
       "7   QTJ90974.1  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...   \n",
       "8   QTY83052.1  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...   \n",
       "9   QUD52764.1  MFVFLVLLPLVSSQCVNLRTRTQLPPAYTNSFTRGVYYPDKVFRSS...   \n",
       "10  QWE88920.1  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...   \n",
       "11  UFO69279.1  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...   \n",
       "12  UOZ45804.1  MFVFLVLLPLVSSQCVNLITRTQSYTNSFTRGVYYPDKVFRSSVLH...   \n",
       "13  UPP14409.1  MFGFLVLLPLVSSQCVNLITRTQSYTNSFTRGVYYPDKVFRSSVLH...   \n",
       "14  UWM38596.1  MFVFLVLLPLVSSQCVNLITRTQSYTNSFTRGVYYPDKVFRSSVLH...   \n",
       "15  UZG29433.1  MFVFLVLLPLVSSQCVNLITRTQSYTNSFTRGVYYPDKVFRSSVLH...   \n",
       "16  WGM84363.1  MFVFLVLLPLVSSQCVNLITRTQSYTNSFTRGVYYPDKVFRSSVLH...   \n",
       "17  WRK13149.1  MFVFLVLLPLVSSQCVNLITTTQSYTNSFTRGVYYPDKVFRSSVLH...   \n",
       "\n",
       "   virus_host_name  \n",
       "0     homo sapiens  \n",
       "1     homo sapiens  \n",
       "2     homo sapiens  \n",
       "3     homo sapiens  \n",
       "4     homo sapiens  \n",
       "5     homo sapiens  \n",
       "6     homo sapiens  \n",
       "7     homo sapiens  \n",
       "8     homo sapiens  \n",
       "9     homo sapiens  \n",
       "10    homo sapiens  \n",
       "11    homo sapiens  \n",
       "12    homo sapiens  \n",
       "13    homo sapiens  \n",
       "14    homo sapiens  \n",
       "15    homo sapiens  \n",
       "16    homo sapiens  \n",
       "17    homo sapiens  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file_path = os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"input/data/coronaviridae/20240313/sarscov2/uniprot/variants/sarscov2_variants_s.csv\")\n",
    "sarscov2_variants_df = pd.read_csv(input_file_path)\n",
    "wiv04_seq_df = sarscov2_variants_df[sarscov2_variants_df[\"id\"] == \"WIV04\"]\n",
    "sarscov2_variants_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c88a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping labels using config : {'Pig': ['sus scrofa'], 'Human': ['homo sapiens'], 'Cat': ['felis catus'], 'Dromedary camel': ['camelus dromedarius'], 'Cattle': ['bos taurus'], 'Chicken': ['gallus gallus'], 'Gray wolf': ['canis lupus'], 'Yak': ['bos grunniens']}\n",
      "label_idx_map={'Cat': 0, 'Cattle': 1, 'Chicken': 2, 'Dromedary camel': 3, 'Gray wolf': 4, 'Human': 5, 'Pig': 6, 'Yak': 7}\n",
      "idx_label_map={0: 'Cat', 1: 'Cattle', 2: 'Chicken', 3: 'Dromedary camel', 4: 'Gray wolf', 5: 'Human', 6: 'Pig', 7: 'Yak'}\n",
      "\n",
      "Sequence tensor size = torch.Size([1, 1269])\n",
      "Sequence = tensor([[13., 14., 22.,  ...,  9., 21., 19.]], dtype=torch.float64)\n",
      "Label tensor size = torch.Size([1])\n",
      "Label = tensor([5])\n"
     ]
    }
   ],
   "source": [
    "index_label_map, dataset_loader = dataset_utils.load_dataset_with_df(sarscov2_variants_df, sequence_settings, label_settings, label_col=label_settings[\"label_col\"], classification_type=\"multi\")\n",
    "print_dataset_loader(dataset_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab719dd1",
   "metadata": {},
   "source": [
    "### Load the pre-trained and fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18bd3a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerEncoder(\n",
      "  (embedding): EmbeddingLayer(\n",
      "    (token_embedding): Embedding(30, 512)\n",
      "    (positional_embedding): PositionalEncoding()\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNormalization()\n",
      "  )\n",
      ")\n",
      "Number of parameters =  12619776\n"
     ]
    }
   ],
   "source": [
    "pre_train_encoder_settings = model[\"pre_train_settings\"]\n",
    "pre_train_encoder_settings[\"vocab_size\"] = constants.VOCAB_SIZE\n",
    "pre_trained_encoder_model = transformer.get_transformer_encoder(pre_train_encoder_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0031a314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerAttention(\n",
      "  (pre_trained_model): TransformerEncoder(\n",
      "    (embedding): EmbeddingLayer(\n",
      "      (token_embedding): Embedding(30, 512)\n",
      "      (positional_embedding): PositionalEncoding()\n",
      "    )\n",
      "    (encoder): Encoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNormalization()\n",
      "    )\n",
      "  )\n",
      "  (self_attn): MultiHeadAttention(\n",
      "    (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (feed_forward): FeedForwardLayer(\n",
      "    (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  )\n",
      "  (linear_ip): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (batch_norm_ip): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear_hidden): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (batch_norm_hidden): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear_hidden_n): ModuleList(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (batch_norm_hidden_n): ModuleList(\n",
      "    (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (linear_op): Linear(in_features=1024, out_features=8, bias=True)\n",
      ")\n",
      "Number of parameters =  18411016\n"
     ]
    }
   ],
   "source": [
    "model[\"pre_trained_model\"] = pre_trained_encoder_model\n",
    "model[\"segment_len\"] = sequence_settings[\"max_sequence_length\"]\n",
    "prediction_model = transformer_attention.get_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fe14fd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/interpretability/../../../output/raw/coronaviridae_s_prot_uniprot_embl_vertebrates_t0.01_c8/20240823/host_multi/fine_tuning_hybrid_cls/mlm_tfenc_l6_h8_lr1e-4_uniref90viridae_msl128b1024vs30cls_s64_hybrid_attention_s64_fnn_2l_d1024_lr1e-4_itr4.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2808860/623784565.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"..\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"..\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"..\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output/raw/coronaviridae_s_prot_uniprot_embl_vertebrates_t0.01_c8/20240823/host_multi/fine_tuning_hybrid_cls/mlm_tfenc_l6_h8_lr1e-4_uniref90viridae_msl128b1024vs30cls_s64_hybrid_attention_s64_fnn_2l_d1024_lr1e-4_itr4.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprediction_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/interpretability/../../../output/raw/coronaviridae_s_prot_uniprot_embl_vertebrates_t0.01_c8/20240823/host_multi/fine_tuning_hybrid_cls/mlm_tfenc_l6_h8_lr1e-4_uniref90viridae_msl128b1024vs30cls_s64_hybrid_attention_s64_fnn_2l_d1024_lr1e-4_itr4.pth'"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"output/raw/coronaviridae_s_prot_uniprot_embl_vertebrates_t0.01_c8/20240823/host_multi/fine_tuning_hybrid_cls/mlm_tfenc_l6_h8_lr1e-4_uniref90viridae_msl128b1024vs30cls_s64_hybrid_attention_s64_fnn_2l_d1024_lr1e-4_itr4.pth\")\n",
    "prediction_model.load_state_dict(torch.load(model_path, map_location=nn_utils.get_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e77de3",
   "metadata": {},
   "source": [
    "### t-SNE Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f6a14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(model, dataset_loader):\n",
    "    model.eval()\n",
    "    seq_dfs = []\n",
    "    for _, record in enumerate(dataset_loader):\n",
    "        seq, label = record\n",
    "        output = model(seq)\n",
    "        # embedding = value for each dimension = mean of the dimensional values of all tokens in the input sequence\n",
    "        seq_encoding = model.embedding\n",
    "        seq_df = pd.DataFrame(seq_encoding.squeeze().cpu().detach().numpy())\n",
    "        seq_df[\"label\"] = label.squeeze().cpu().detach().numpy()\n",
    "        print(seq_df.shape)\n",
    "        seq_dfs.append(seq_df)\n",
    "    df = pd.concat(seq_dfs)\n",
    "    print(df.shape)\n",
    "    return df\n",
    "\n",
    "def view_tsne_representation(rep_df, index_label_map):\n",
    "    columns = rep_df.columns\n",
    "    print(columns)\n",
    "    X = rep_df[range(512)]\n",
    "    tsne_model = TSNE(n_components=2, verbose=1, init=\"pca\", learning_rate=\"auto\").fit(X)\n",
    "    X_tsne_emb = pd.DataFrame(tsne_model.fit_transform(X))\n",
    "    print(X_tsne_emb.shape)\n",
    "    print(X_tsne_emb)\n",
    "    X_tsne_emb[\"label\"] = rep_df[\"label\"].values\n",
    "    X_tsne_emb[\"label\"] = X_tsne_emb[\"label\"].map(index_label_map)\n",
    "    \n",
    "    sns.scatterplot(data = X_tsne_emb, x=0, y=1, hue=\"label\")\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.show()\n",
    "    return tsne_model, X_tsne_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba83ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emb_df = compute_embeddings(prediction_model, dataset_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e854b6",
   "metadata": {},
   "source": [
    "### Attention value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2032bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attention_of_sequence(model, seq):\n",
    "    print(f\"sequence length = {seq_len}\")\n",
    "    model.eval()\n",
    "    output = model(seq)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarscov2_variants_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5489d0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_settings[\"batch_size\"] = 1\n",
    "sequence_settings[\"max_sequence_length\"] = 256\n",
    "\n",
    "_, wiv04_seq_df_dataset_loader = dataset_utils.load_dataset_with_df(wiv04_seq_df, sequence_settings, label_settings, label_col=label_settings[\"label_col\"], classification_type=\"multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2d2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model.eval()\n",
    "input, label = next(iter(dataset_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969712f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in iter(dataset_loader):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1709fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "input, label = next(iter(dataset_loader))\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7e5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef34d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = prediction_model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7479b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa2285",
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af274ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464f3c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model.pre_trained_model.encoder.layers[-1].self_attn.self_attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a490a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model.self_attn.self_attn.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dec741",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_mapping = {}\n",
    "j = 0\n",
    "for i in range(1, len(wiv04_seq_df[\"seq\"][0])+1, 64):\n",
    "    try:\n",
    "        start = i\n",
    "        # end = seq_len if i+127 > seq_len else i+127\n",
    "        end = i + 255\n",
    "        pos_mapping[j] = f\"{j}: {start}-{end}\"\n",
    "    except KeyError:\n",
    "        break\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9063a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aee9ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_seg_attn = prediction_model.self_attn.self_attn.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb97f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.rcParams[\"xtick.labelsize\"] = 40\n",
    "plt.rcParams[\"ytick.labelsize\"] = 40\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "fig, axs = plt.subplots(4, 2, figsize=(80, 100), sharex=False, sharey=True)\n",
    "\n",
    "c = 0\n",
    "for i in range(4):\n",
    "    for j in range(2):\n",
    "        df = pd.DataFrame(inter_seg_attn[c].squeeze().detach().cpu().numpy())\n",
    "        df.rename(columns=pos_mapping, inplace=True)\n",
    "        df.rename(index=pos_mapping, inplace=True)\n",
    "        sns.heatmap(df, ax=axs[i, j], linewidth=.1, vmin=0, vmax=1)\n",
    "        axs[i, j].set_title(f\"Head {c}\")\n",
    "        c += 1\n",
    "\n",
    "plt.tight_layout(pad=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad22c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_seg_index = 0\n",
    "intra_seg_attn = prediction_model.pre_trained_model.encoder.layers[-1].self_attn.self_attn[intra_seg_index].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a03009",
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_seg_attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f191f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_seg_pos_map = {}\n",
    "intra_seg_start = intra_seg_index * 64 + 1\n",
    "intra_seg_end = intra_seg_start + 256\n",
    "\n",
    "c = 0\n",
    "for i in range(intra_seg_start, intra_seg_end):\n",
    "    intra_seg_pos_map[c] = i\n",
    "    c += 1\n",
    "\n",
    "intra_seg_pos_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d18c5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.rcParams[\"xtick.labelsize\"] = 40\n",
    "plt.rcParams[\"ytick.labelsize\"] = 40\n",
    "plt.rcParams.update({'font.size': 40})\n",
    "fig, axs = plt.subplots(4, 2, figsize=(80, 100), sharex=False, sharey=False)\n",
    "\n",
    "c = 0\n",
    "for i in range(4):\n",
    "    for j in range(2):\n",
    "        df = pd.DataFrame(intra_seg_attn[c].squeeze().detach().cpu().numpy())\n",
    "        df.rename(columns=intra_seg_pos_map, inplace=True)\n",
    "        df.rename(index=intra_seg_pos_map, inplace=True)\n",
    "        sns.heatmap(df, ax=axs[i, j], linewidth=.1)\n",
    "        axs[i, j].set_title(f\"Head {c}\")\n",
    "        c += 1\n",
    "\n",
    "plt.tight_layout(pad=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90db1713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
