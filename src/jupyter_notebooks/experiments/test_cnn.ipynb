{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c9e9aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/grads/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/experiments',\n",
       " '/home/grads/blessyantony/anaconda3/envs/zoonosis/lib/python310.zip',\n",
       " '/home/grads/blessyantony/anaconda3/envs/zoonosis/lib/python3.10',\n",
       " '/home/grads/blessyantony/anaconda3/envs/zoonosis/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/home/grads/blessyantony/anaconda3/envs/zoonosis/lib/python3.10/site-packages',\n",
       " '/home/grads/blessyantony/anaconda3/envs/zoonosis/lib/python3.10/site-packages/PyQt5_sip-12.11.0-py3.10-linux-x86_64.egg',\n",
       " '/home/grads/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/experiments/../../..',\n",
       " '/home/grads/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/experiments/../..',\n",
       " '/home/grads/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/experiments/..']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\", \"..\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e3d5a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torchvision.datasets\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Conv2d\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "from utils import utils, nn_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0b7f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_2D_Model(nn.Module):\n",
    "    def __init__(self, n_classes, N, n_filters, kernel_size, stride, img_size):\n",
    "        super(CNN_2D_Model, self).__init__()\n",
    "        # padding: same ensures the output has the same size as the input\n",
    "        self.conv2d = Conv2d(in_channels=3,\n",
    "                             out_channels=n_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=\"same\")\n",
    "        self.conv2d_hidden = Conv2d(in_channels=n_filters,\n",
    "                                    out_channels=n_filters,\n",
    "                                    kernel_size=kernel_size,\n",
    "                                    stride=stride,\n",
    "                                    padding=\"same\")\n",
    "        # intermediate hidden layers (number = N-1): hidden_dim --> hidden_dim\n",
    "        # N-1 because we already have one layer converting input_dim --> hidden_dim\n",
    "        self.conv2d_hidden_layers = nn_utils.create_clones(self.conv2d_hidden, N - 1)\n",
    "        self.linear = nn.Linear(img_size * img_size * n_filters, n_classes)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.conv2d(X))\n",
    "        for conv2d_hidden_layer in self.conv2d_hidden_layers:\n",
    "            X = F.relu(conv2d_hidden_layer(X))\n",
    "\n",
    "        # aggregate the embeddings from cnn\n",
    "        # mean of the representations of all tokens\n",
    "        self.cnn_emb = torch.flatten(X, 1)  # flatten all dimensions except batch\n",
    "        y = self.linear(self.cnn_emb)\n",
    "        return y\n",
    "\n",
    "\n",
    "def get_cnn_model(model):\n",
    "    cnn_model = CNN_2D_Model(n_classes=model[\"n_classes\"],\n",
    "                             N=model[\"depth\"],\n",
    "                             n_filters=model[\"n_filters\"],\n",
    "                             kernel_size=model[\"kernel_size\"],\n",
    "                             stride=model[\"stride\"],\n",
    "                             img_size=model[\"img_size\"])\n",
    "\n",
    "    print(cnn_model)\n",
    "    print(\"Number of parameters = \", sum(p.numel() for p in cnn_model.parameters() if p.requires_grad))\n",
    "    return cnn_model.to(nn_utils.get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15e8238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, train_dataset_loader, test_dataset_loader, loss, n_epochs, model_name, mode):\n",
    "    tbw = SummaryWriter()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    lr_scheduler = OneCycleLR(\n",
    "        optimizer=optimizer,\n",
    "        max_lr=1e-4,\n",
    "        epochs=n_epochs,\n",
    "        steps_per_epoch=len(train_dataset_loader),\n",
    "        pct_start=0.1,\n",
    "        anneal_strategy='cos',\n",
    "        div_factor=25.0,\n",
    "        final_div_factor=10000.0)\n",
    "    model.train_iter = 0\n",
    "    model.test_iter = 0\n",
    "    if mode == \"train\":\n",
    "        # train the model only if set to train mode\n",
    "        for e in range(n_epochs):\n",
    "            model = run_epoch(model, train_dataset_loader, test_dataset_loader, criterion, optimizer,\n",
    "                              lr_scheduler, tbw, model_name, e)\n",
    "\n",
    "    return evaluate_model(model, test_dataset_loader, criterion, tbw, model_name, epoch=None, log_loss=False), model\n",
    "\n",
    "\n",
    "def run_epoch(model, train_dataset_loader, test_dataset_loader, criterion, optimizer, lr_scheduler, tbw, model_name,\n",
    "              epoch):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for _, record in enumerate(pbar := tqdm.tqdm(train_dataset_loader)):\n",
    "        input, label = record\n",
    "        input = input.to(nn_utils.get_device())\n",
    "        label = label.to(nn_utils.get_device())\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(input)\n",
    "        output = output.to(nn_utils.get_device())\n",
    "\n",
    "        loss = criterion(output, label.long())\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        model.train_iter += 1\n",
    "        curr_lr = lr_scheduler.get_last_lr()[0]\n",
    "        train_loss = loss.item()\n",
    "        tbw.add_scalar(f\"{model_name}/learning-rate\", float(curr_lr), model.train_iter)\n",
    "        tbw.add_scalar(f\"{model_name}/training-loss\", float(train_loss), model.train_iter)\n",
    "        pbar.set_description(\n",
    "            f\"{model_name}/training-loss = {float(train_loss)}, model.n_iter={model.train_iter}, epoch={epoch + 1}\")\n",
    "\n",
    "    # Testing\n",
    "    evaluate_model(model, test_dataset_loader, criterion, tbw, model_name, epoch, log_loss=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_dataset_loader, criterion, tbw, model_name, epoch, log_loss=False):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        results = []\n",
    "        for _, record in enumerate(pbar := tqdm.tqdm(test_dataset_loader)):\n",
    "            input, label = record\n",
    "            input = input.to(nn_utils.get_device())\n",
    "            label = label.to(nn_utils.get_device())\n",
    "            output = model(input)  # b x n_classes\n",
    "            output = output.to(nn_utils.get_device())\n",
    "\n",
    "            loss = criterion(output, label.long())\n",
    "            val_loss = loss.item()\n",
    "            model.test_iter += 1\n",
    "            if log_loss:\n",
    "                tbw.add_scalar(f\"{model_name}/validation-loss\", float(val_loss), model.test_iter)\n",
    "                pbar.set_description(\n",
    "                    f\"{model_name}/validation-loss = {float(val_loss)}, model.n_iter={model.test_iter}, epoch={epoch + 1}\")\n",
    "            # to get probabilities of the output\n",
    "            output = F.softmax(output, dim=-1)\n",
    "            result_df = pd.DataFrame(output.cpu().numpy())\n",
    "            result_df[\"y_true\"] = label.cpu().numpy()\n",
    "            results.append(result_df)\n",
    "    return pd.concat(results, ignore_index=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Main\")\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    batch_size = 64\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "    train_dataset_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataset_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    classes = (\"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")\n",
    "\n",
    "    model = get_cnn_model({\n",
    "        \"n_classes\": 10,\n",
    "        \"depth\": 2,\n",
    "        \"n_filters\": 3,\n",
    "        \"kernel_size\": 3,\n",
    "        \"stride\": 1,\n",
    "        \"img_size\": 32\n",
    "    })\n",
    "\n",
    "    results_df, _ = run_model(model=model,\n",
    "              train_dataset_loader=train_dataset_loader,\n",
    "              test_dataset_loader=test_dataset_loader,\n",
    "              loss=\"FocalLoss\",\n",
    "              n_epochs=10,\n",
    "              model_name=\"CNN-CIFAR10\",\n",
    "              mode=\"train\")\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be306532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CNN_2D_Model(\n",
      "  (conv2d): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (conv2d_hidden): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (conv2d_hidden_layers): ModuleList(\n",
      "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  )\n",
      "  (linear): Linear(in_features=3072, out_features=10, bias=True)\n",
      ")\n",
      "Number of parameters =  30982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CNN-CIFAR10/training-loss = 1.9543399810791016, model.n_iter=782, epoch=1: 100%|██████████| 782/782 [00:12<00:00, 62.06it/s]\n",
      "CNN-CIFAR10/validation-loss = 1.7199288606643677, model.n_iter=157, epoch=1: 100%|████████| 157/157 [00:02<00:00, 72.44it/s]\n",
      "CNN-CIFAR10/training-loss = 1.9425971508026123, model.n_iter=1564, epoch=2: 100%|█████████| 782/782 [00:12<00:00, 64.64it/s]\n",
      "CNN-CIFAR10/validation-loss = 1.6346534490585327, model.n_iter=314, epoch=2: 100%|████████| 157/157 [00:01<00:00, 79.91it/s]\n",
      "CNN-CIFAR10/training-loss = 2.3432414531707764, model.n_iter=2346, epoch=3: 100%|█████████| 782/782 [00:12<00:00, 64.67it/s]\n",
      "CNN-CIFAR10/validation-loss = 1.656532883644104, model.n_iter=471, epoch=3: 100%|█████████| 157/157 [00:02<00:00, 71.54it/s]\n",
      "CNN-CIFAR10/training-loss = 1.6092811822891235, model.n_iter=3128, epoch=4: 100%|█████████| 782/782 [00:12<00:00, 63.38it/s]\n",
      "CNN-CIFAR10/validation-loss = 1.6673660278320312, model.n_iter=628, epoch=4: 100%|████████| 157/157 [00:02<00:00, 72.45it/s]\n",
      "CNN-CIFAR10/training-loss = 1.6811550855636597, model.n_iter=3910, epoch=5: 100%|█████████| 782/782 [00:12<00:00, 60.85it/s]\n",
      "CNN-CIFAR10/validation-loss = 1.635848045349121, model.n_iter=785, epoch=5: 100%|█████████| 157/157 [00:02<00:00, 73.19it/s]\n",
      "CNN-CIFAR10/training-loss = 1.4216920137405396, model.n_iter=4692, epoch=6: 100%|█████████| 782/782 [00:12<00:00, 63.97it/s]\n",
      "CNN-CIFAR10/validation-loss = 1.653052806854248, model.n_iter=942, epoch=6: 100%|█████████| 157/157 [00:02<00:00, 75.26it/s]\n",
      "CNN-CIFAR10/training-loss = 1.583945393562317, model.n_iter=5474, epoch=7: 100%|██████████| 782/782 [00:12<00:00, 63.08it/s]\n",
      "CNN-CIFAR10/validation-loss = 1.6553473472595215, model.n_iter=1099, epoch=7: 100%|███████| 157/157 [00:02<00:00, 72.47it/s]\n",
      "CNN-CIFAR10/training-loss = 2.0730607509613037, model.n_iter=6256, epoch=8: 100%|█████████| 782/782 [00:12<00:00, 60.65it/s]\n",
      "CNN-CIFAR10/validation-loss = 1.6673572063446045, model.n_iter=1256, epoch=8: 100%|███████| 157/157 [00:02<00:00, 67.21it/s]\n",
      "CNN-CIFAR10/training-loss = 1.474568247795105, model.n_iter=7038, epoch=9: 100%|██████████| 782/782 [00:12<00:00, 60.23it/s]\n",
      "CNN-CIFAR10/validation-loss = 1.6639941930770874, model.n_iter=1413, epoch=9: 100%|███████| 157/157 [00:02<00:00, 73.43it/s]\n",
      "CNN-CIFAR10/training-loss = 1.994061827659607, model.n_iter=7820, epoch=10: 100%|█████████| 782/782 [00:13<00:00, 59.72it/s]\n",
      "CNN-CIFAR10/validation-loss = 1.6584808826446533, model.n_iter=1570, epoch=10: 100%|██████| 157/157 [00:02<00:00, 73.49it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 157/157 [00:01<00:00, 82.59it/s]\n"
     ]
    }
   ],
   "source": [
    "results_df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe01b37d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029aa509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
