{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12e40648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, accuracy_score, f1_score, auc, precision_recall_curve\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf406038",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a6fe47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "classes = (\"plane\", \"car\", \"bird\", \"cat\",\n",
    "           \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")\n",
    "max_lr = 1e-3\n",
    "n_epochs = 5\n",
    "model_name = \"CNN-2D\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df50933",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98ceb1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                ])\n",
    "train_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "train_dataset_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=True)                                   \n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "test_dataset_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                                  batch_size=batch_size, \n",
    "                                                  shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1848e3b8",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73556a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_1 = nn.Conv2d(3, 6, 5, stride=1, padding=\"same\")\n",
    "        self.conv_2 = nn.Conv2d(6, 16, 5, stride=1, padding=\"same\")\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        self.linear_1 = nn.Linear(16 * 8 * 8, 120)\n",
    "        self.linear_2 = nn.Linear(120, 84)\n",
    "        self.linear_3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(F.relu(self.conv_1(x)))\n",
    "        x = self.maxpool(F.relu(self.conv_2(x)))\n",
    "        \n",
    "        x = torch.flatten(x, 1) # flatten all dimension except the first one which corresponds to batch\n",
    "        \n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = F.relu(self.linear_2(x))\n",
    "        \n",
    "        x = self.linear_3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d65bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d923405",
   "metadata": {},
   "source": [
    "### Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d3d0d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = OneCycleLR(\n",
    "                    optimizer=optimizer,\n",
    "                    max_lr=max_lr,\n",
    "                    epochs=n_epochs,\n",
    "                    steps_per_epoch=len(train_dataset_loader),\n",
    "                    pct_start=0.1,\n",
    "                    anneal_strategy='cos',\n",
    "                    div_factor=25.0,\n",
    "                    final_div_factor=10000.0)\n",
    "tbw = SummaryWriter() # tensorboard summary writer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0236d39b",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "126d9dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train_iter = 0\n",
    "    model.val_iter = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for _, data in enumerate(pbar := tqdm.tqdm(train_dataset_loader, 0)):\n",
    "            model.train_iter += 1 \n",
    "            # get input and labels; data is a list of [(inputs, labels)]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            # loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # backward propagate loss\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            # training_loss\n",
    "            train_loss = loss.item()\n",
    "            # tensorboard + logs\n",
    "            tbw.add_scalar(f\"{model_name}/training-loss\", float(train_loss), model.train_iter)\n",
    "            pbar.set_description(f\"{model_name}/training-loss={train_loss}, steps={model.train_iter}, epoch={epoch+1}\")\n",
    "            \n",
    "        validate(model, epoch)\n",
    "        \n",
    "    return model, validate(model, epoch)\n",
    "    \n",
    "def validate(model, epoch=0):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for _, data in enumerate(pbar := tqdm.tqdm(test_dataset_loader, 0)):\n",
    "            # get input and labels; data is a list of [(inputs, labels)]\n",
    "            inputs, labels = data\n",
    "\n",
    "            output = model(inputs)\n",
    "            # loss\n",
    "            loss = criterion(output, labels)\n",
    "            curr_val_loss = loss.item()\n",
    "            model.val_iter += 1\n",
    "\n",
    "            # tensorboard + logs\n",
    "            tbw.add_scalar(f\"{model_name}/validation-loss\", float(curr_val_loss), model.val_iter)\n",
    "            pbar.set_description(f\"{model_name}/validation-loss={curr_val_loss}, steps={model.val_iter}, epoch={epoch+1}\")\n",
    "\n",
    "            # to get probabilities of the output\n",
    "            output = F.softmax(output, dim=-1)\n",
    "            result_df = pd.DataFrame(output.cpu().numpy())\n",
    "            result_df[\"y_true\"] = labels.cpu().numpy()\n",
    "            results.append(result_df)\n",
    "    \n",
    "    return pd.concat(results, ignore_index=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f3d96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CNN-2D/training-loss=2.3098697662353516, steps=210, epoch=1:  27%|████████████████▊                                              | 209/782 [00:46<02:12,  4.32it/s]"
     ]
    }
   ],
   "source": [
    "model, results = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd2e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46b528b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUPRC for class 0 = 0.21005013117941906\n",
      "AUPRC for class 1 = 0.18453593516647848\n",
      "AUPRC for class 2 = 0.1310956042972487\n",
      "AUPRC for class 3 = 0.13474339035829364\n",
      "AUPRC for class 4 = 0.14889935264642115\n",
      "AUPRC for class 5 = 0.12173970517548381\n",
      "AUPRC for class 6 = 0.24230072657091292\n",
      "AUPRC for class 7 = 0.20533794780987127\n",
      "AUPRC for class 8 = 0.1814796143209802\n",
      "AUPRC for class 9 = 0.19078316126738415\n",
      "Macro AUPRC = 0.17509655687924933\n"
     ]
    }
   ],
   "source": [
    "auprcs = []\n",
    "for i in range(10):\n",
    "    precision, recall, _ = precision_recall_curve(y_true=results[\"y_true\"].values, probas_pred=results[i].values, pos_label=i)\n",
    "    auprc = auc(recall, precision)\n",
    "    print(f\"AUPRC for class {i} = {auprc}\")\n",
    "    auprcs.append(auprc)\n",
    "\n",
    "macro_auprc = mean(auprcs)\n",
    "print(f\"Macro AUPRC = {macro_auprc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61be75e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
