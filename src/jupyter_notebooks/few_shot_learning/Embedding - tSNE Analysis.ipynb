{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c13ffe",
   "metadata": {},
   "source": [
    "## TSNE Analysis of VirProBERT Few Shot Classifier\n",
    "### Dataset: EMBL mapping, Vertebrates, Non-IDV\n",
    "\n",
    "**Models**: VirProBERT, Few-Shot Classifier\n",
    "\n",
    "**Maximum Sequence Length**: 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b73dcb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/few_shot_learning',\n",
       " '/opt/conda/lib/python38.zip',\n",
       " '/opt/conda/lib/python3.8',\n",
       " '/opt/conda/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/home/blessyantony/.local/lib/python3.8/site-packages',\n",
       " '/opt/conda/lib/python3.8/site-packages',\n",
       " '/opt/conda/lib/python3.8/site-packages/IPython/extensions',\n",
       " '/home/blessyantony/.ipython',\n",
       " '/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/few_shot_learning/../../../../..',\n",
       " '/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/few_shot_learning/../../../..',\n",
       " '/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/few_shot_learning/../../..',\n",
       " '/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/few_shot_learning/../..']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"..\", \"..\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"..\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\", \"..\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deda438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import utils, nn_utils, dataset_utils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from src.utils import utils, nn_utils\n",
    "from src.models.nlp.transformer import transformer\n",
    "from src.transfer_learning.fine_tuning import host_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abc115e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "virprobert_model_file_path = os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"output/raw/uniref90_embl_vertebrates_non_idv_t0.01_c5/20240611/host_multi/fine_tuning/host_prediction_fnn_2l_d1024_lr1e-4_itr0.pth\")\n",
    "virprobert_few_shot_classifier_model_file_path = os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"output/raw/uniref90_embl_vertebrates_non_idv/20240612/host_multi/few_shot_learning/virprobert_itr2.pth\")\n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"input/data/uniref90/20240131\")\n",
    "input_file_names = [\"uniref90_viridae_embl_hosts_pruned_metadata_species_vertebrates_w_seq_non_idv_lt_1percent_prevalence.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74d69057",
   "metadata": {},
   "outputs": [],
   "source": [
    "virprobert_settings = {\n",
    "    \"mlm_encoder_settings\": {\n",
    "        \"embedding\": \"linear\",\n",
    "        \"n_heads\": 8,\n",
    "        \"depth\": 6,\n",
    "        \"input_dim\": 512, # input embedding dimension\n",
    "        \"hidden_dim\": 1024,\n",
    "        \"n_tokens\": 28\n",
    "    },\n",
    "    \"host_prediction_settings\": {\n",
    "        \"depth\": 2,\n",
    "        \"n_classes\": 5,\n",
    "        \"input_dim\": 512, # input embedding dimension\n",
    "        \"hidden_dim\": 1024\n",
    "    }\n",
    "}\n",
    "\n",
    "sequence_settings= {\n",
    "    \"id_col\": \"uniref90_id\",\n",
    "    \"sequence_col\": \"seq\",\n",
    "    \"max_sequence_length\": 2048, # 6630 # 1024 # 1115\n",
    "    \"truncate\": True,\n",
    "    \"split_sequence\": False,\n",
    "    \"pad_token_val\": 0,\n",
    "    \"feature_type\": \"token\",\n",
    "    \"batch_size\": 64,\n",
    "}\n",
    "\n",
    "label_settings= {\n",
    "    \"label_col\": \"virus_host_name\"\n",
    "}\n",
    "max_seq_len = sequence_settings[\"max_sequence_length\"]\n",
    "virprobert_settings[\"mlm_encoder_settings\"][\"max_seq_len\"] = max_seq_len\n",
    "virprobert_settings[\"host_prediction_settings\"][\"max_seq_len\"] = max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a08c9286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerEncoder(\n",
      "  (embedding): EmbeddingLayer(\n",
      "    (token_embedding): Embedding(28, 512)\n",
      "    (positional_embedding): PositionalEncoding()\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): NormalizationLayer()\n",
      "  )\n",
      ")\n",
      "Number of parameters =  12618752\n",
      "HostPrediction(\n",
      "  (pre_trained_model): TransformerEncoder(\n",
      "    (embedding): EmbeddingLayer(\n",
      "      (token_embedding): Embedding(28, 512)\n",
      "      (positional_embedding): PositionalEncoding()\n",
      "    )\n",
      "    (encoder): Encoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm): NormalizationLayer()\n",
      "    )\n",
      "  )\n",
      "  (linear_ip): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (linear_hidden): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (linear_hidden_n): ModuleList(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear_op): Linear(in_features=1024, out_features=5, bias=True)\n",
      ")\n",
      "Number of parameters =  16297989\n"
     ]
    }
   ],
   "source": [
    "mlm_encoder_model = transformer.get_transformer_encoder(virprobert_settings[\"mlm_encoder_settings\"])\n",
    "virprobert_settings[\"host_prediction_settings\"][\"pre_trained_model\"] = mlm_encoder_model\n",
    "pre_trained_model = host_prediction.get_host_prediction_model(virprobert_settings[\"host_prediction_settings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cf2a9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained_model.load_state_dict(torch.load(virprobert_model_file_path, map_location=nn_utils.get_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33ce1972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(input_dir, input_file_names, sequence_settings):\n",
    "    df = dataset_utils.read_dataset(input_dir, input_file_names, cols=[sequence_settings[\"sequence_col\"], label_settings[\"label_col\"]])\n",
    "    label_idx_map, idx_label_map = utils.get_label_vocabulary(list(df[label_settings[\"label_col\"]].unique()))\n",
    "    df.replace({label_settings[\"label_col\"]:label_idx_map}, inplace=True)\n",
    "    dataset_loader = dataset_utils.get_dataset_loader(df, sequence_settings, label_settings[\"label_col\"])\n",
    "    print(df.head())\n",
    "    return dataset_loader\n",
    "\n",
    "def compute_dataset_representations_fnn(model, dataset_loader):\n",
    "    model.eval()\n",
    "    seq_dfs = []\n",
    "    for _, record in enumerate(dataset_loader):\n",
    "        seq, label = record\n",
    "        seq_encoding = model.get_embedding(seq)\n",
    "        seq_df = pd.DataFrame(seq_encoding.squeeze().cpu().detach().numpy())\n",
    "        seq_df[\"label\"] = label.squeeze().cpu().detach().numpy()\n",
    "        seq_dfs.append(seq_df)\n",
    "    df = pd.concat(seq_dfs)\n",
    "    print(df.shape)\n",
    "    return df\n",
    "\n",
    "def print_dataset_loader(dataset_loader):\n",
    "    sequence, label = next(iter(dataset_loader))\n",
    "    print(sequence.shape)\n",
    "    print(sequence)\n",
    "    print(label.shape)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b537bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input file: /home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/few_shot_learning/../../../input/data/uniref90/20240131/uniref90_viridae_embl_hosts_pruned_metadata_species_vertebrates_w_seq_non_idv_lt_1percent_prevalence.csv, size = (16074, 2)\n",
      "Size of input dataset = (16074, 2)\n",
      "   virus_host_name                                                seq\n",
      "0              273  VEFLKRNKVYFMNRQDVLDKNHVADIDKLIDYAASGDPTSPEDIES...\n",
      "1             1027  EPTGQTADWLTIIIYLTSFVIPIILKALYMLTTRGRQTTKDNKGMR...\n",
      "2             1176  MEAADILDGLNWEHLVESHYRTDAALDTQAFFHVNREKGDGNCFFR...\n",
      "3              410  MGQTVTTPLSLTLDHWSEVRTRAHNQGVEVRKKKWVTLCEAEWVIM...\n",
      "4             1178  MAHSHHNHCLSACALERECNELGVCLPARLRGFESLTCDPKPPTPS...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Padding' object has no attribute 'max_seq_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1001802/3220889667.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_file_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint_dataset_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1001802/125652192.py\u001b[0m in \u001b[0;36mprint_dataset_loader\u001b[0;34m(dataset_loader)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_dataset_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/dev/git/zoonosis/src/jupyter_notebooks/few_shot_learning/../../datasets/collations/padding.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mpadded_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpadded_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Padding' object has no attribute 'max_seq_length'"
     ]
    }
   ],
   "source": [
    "dataset_loader = load_dataset(input_dir, input_file_names, sequence_settings)\n",
    "print_dataset_loader(dataset_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fffdf4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
