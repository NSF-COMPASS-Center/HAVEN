{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c13ffe",
   "metadata": {},
   "source": [
    "## TSNE Analysis of VirProBERT Few Shot Classifier\n",
    "### Dataset: EMBL mapping, Vertebrates, Non-IDV\n",
    "\n",
    "**Models**: VirProBERT, Few-Shot Classifier\n",
    "\n",
    "**Maximum Sequence Length**: segment len 256, stride 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b73dcb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/few_shot_learning',\n",
       " '/opt/conda/lib/python38.zip',\n",
       " '/opt/conda/lib/python3.8',\n",
       " '/opt/conda/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/home/blessyantony/.local/lib/python3.8/site-packages',\n",
       " '/opt/conda/lib/python3.8/site-packages',\n",
       " '/opt/conda/lib/python3.8/site-packages/IPython/extensions',\n",
       " '/home/blessyantony/.ipython',\n",
       " '/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/few_shot_learning/../../../../..',\n",
       " '/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/few_shot_learning/../../../..',\n",
       " '/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/few_shot_learning/../../..',\n",
       " '/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/few_shot_learning/../..']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"..\", \"..\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"..\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\", \"..\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deda438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets.collations.padding import Padding\n",
    "\n",
    "from src.utils import constants, nn_utils, utils\n",
    "from src.models.virprobert import VirProBERT\n",
    "from src.models.baseline.nlp.transformer.transformer import TransformerEncoder\n",
    "from datasets.protein_sequence_dataset import ProteinSequenceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abc115e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "virprobert_model_file_path = os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"output/raw/uniref90_embl_vertebrates_non_idv_t0.01_c5_seq_len_in_99.9percentile/20240826/host_multi/fine_tuning_hybrid_cls/mlm_tfenc_l6_h8_lr1e-4_uniref90viridae_msl256s64allemb_vs30cls_batchnorm_hybrid_attention_msl256s64ae_fnn_2l_d1024_lr1e-4_itr4.pth\")\n",
    "virprobert_few_shot_classifier_model_file_path = os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"output/raw/uniref90_embl_vertebrates_non_idv/20240928/host_multi/few_shot_learning/fsl_tr_w3s5q10_te_w3s5q-1_e100b32_split70-10-20_hybrid-attention_sl256st64vs30cls_fnn_2l_d1024_lr1e-4_itr4.pth\")\n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"input/data/uniref90/20240131\")\n",
    "input_file_names = [\"uniref90_viridae_embl_hosts_pruned_metadata_species_vertebrates_w_seq_non_idv_lt_1_gte_0.05_prcnt_prevalence_seq_len_in_99prcntile.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74d69057",
   "metadata": {},
   "outputs": [],
   "source": [
    "virprobert_settings = {\n",
    "    \"mlm_encoder_settings\": {\n",
    "        \"n_heads\": 8,\n",
    "        \"depth\": 6,\n",
    "        \"input_dim\": 512, # input embedding dimension\n",
    "        \"hidden_dim\": 1024,\n",
    "        \"vocab_size\": constants.VOCAB_SIZE\n",
    "    },\n",
    "    \"host_prediction_settings\": {\n",
    "        \"n_mlp_layers\": 2,\n",
    "        \"n_classes\": 5,\n",
    "        \"input_dim\": 512, # input embedding dimension\n",
    "        \"hidden_dim\": 1024,\n",
    "        \"cls_token\": True,\n",
    "        \"n_heads\": 8,\n",
    "        \"stride\": 64,\n",
    "        \"data_parallel\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "sequence_settings= {\n",
    "    \"id_col\": \"uniref90_id\",\n",
    "    \"sequence_col\": \"seq\",\n",
    "    \"max_sequence_length\": 256,\n",
    "    \"truncate\": False,\n",
    "    \"split_sequence\": False,\n",
    "    \"feature_type\": \"token\",\n",
    "    \"batch_size\": 4,\n",
    "}\n",
    "\n",
    "label_settings= {\n",
    "    \"label_col\": \"virus_host_name\"\n",
    "}\n",
    "max_seq_len = sequence_settings[\"max_sequence_length\"]\n",
    "virprobert_settings[\"mlm_encoder_settings\"][\"max_seq_len\"] = max_seq_len\n",
    "virprobert_settings[\"host_prediction_settings\"][\"segment_len\"] = max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a08c9286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerEncoder(\n",
      "  (embedding): EmbeddingLayer(\n",
      "    (token_embedding): Embedding(30, 512)\n",
      "    (positional_embedding): PositionalEncoding()\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (layer_norm): LayerNormalization()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNormalization()\n",
      "  )\n",
      ")\n",
      "Number of parameters =  12619776\n",
      "VirProBERT(\n",
      "  (linear_ip): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (linear_hidden): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (linear_hidden_n): ModuleList(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear_op): Linear(in_features=1024, out_features=5, bias=True)\n",
      "  (batch_norm_ip): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm_hidden): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm_hidden_n): ModuleList(\n",
      "    (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (pre_trained_model): TransformerEncoder(\n",
      "    (embedding): EmbeddingLayer(\n",
      "      (token_embedding): Embedding(30, 512)\n",
      "      (positional_embedding): PositionalEncoding()\n",
      "    )\n",
      "    (encoder): Encoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (layer_norm): LayerNormalization()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNormalization()\n",
      "    )\n",
      "  )\n",
      "  (self_attn): MultiHeadAttention(\n",
      "    (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (feed_forward): FeedForwardLayer(\n",
      "    (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  )\n",
      ")\n",
      "VirProBERT: Number of parameters =  18407941\n"
     ]
    }
   ],
   "source": [
    "mlm_encoder_model = TransformerEncoder.get_transformer_encoder(virprobert_settings[\"mlm_encoder_settings\"], cls_token=True)\n",
    "virprobert_settings[\"host_prediction_settings\"][\"pre_trained_model\"] = mlm_encoder_model\n",
    "\n",
    "virprobert_model = VirProBERT.get_model(model_params=virprobert_settings[\"host_prediction_settings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cf2a9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VirProBERT(\n",
       "  (linear_ip): Linear(in_features=512, out_features=1024, bias=True)\n",
       "  (linear_hidden): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (linear_hidden_n): ModuleList(\n",
       "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (linear_op): Linear(in_features=1024, out_features=5, bias=True)\n",
       "  (batch_norm_ip): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm_hidden): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm_hidden_n): ModuleList(\n",
       "    (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (pre_trained_model): TransformerEncoder(\n",
       "    (embedding): EmbeddingLayer(\n",
       "      (token_embedding): Embedding(30, 512)\n",
       "      (positional_embedding): PositionalEncoding()\n",
       "    )\n",
       "    (encoder): Encoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): EncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (feed_forward): FeedForwardLayer(\n",
       "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          )\n",
       "          (residual_connections): ModuleList(\n",
       "            (0): ResidualConnectionLayer(\n",
       "              (layer_norm): LayerNormalization()\n",
       "            )\n",
       "            (1): ResidualConnectionLayer(\n",
       "              (layer_norm): LayerNormalization()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): EncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (feed_forward): FeedForwardLayer(\n",
       "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          )\n",
       "          (residual_connections): ModuleList(\n",
       "            (0): ResidualConnectionLayer(\n",
       "              (layer_norm): LayerNormalization()\n",
       "            )\n",
       "            (1): ResidualConnectionLayer(\n",
       "              (layer_norm): LayerNormalization()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): EncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (feed_forward): FeedForwardLayer(\n",
       "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          )\n",
       "          (residual_connections): ModuleList(\n",
       "            (0): ResidualConnectionLayer(\n",
       "              (layer_norm): LayerNormalization()\n",
       "            )\n",
       "            (1): ResidualConnectionLayer(\n",
       "              (layer_norm): LayerNormalization()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): EncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (feed_forward): FeedForwardLayer(\n",
       "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          )\n",
       "          (residual_connections): ModuleList(\n",
       "            (0): ResidualConnectionLayer(\n",
       "              (layer_norm): LayerNormalization()\n",
       "            )\n",
       "            (1): ResidualConnectionLayer(\n",
       "              (layer_norm): LayerNormalization()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): EncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (feed_forward): FeedForwardLayer(\n",
       "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          )\n",
       "          (residual_connections): ModuleList(\n",
       "            (0): ResidualConnectionLayer(\n",
       "              (layer_norm): LayerNormalization()\n",
       "            )\n",
       "            (1): ResidualConnectionLayer(\n",
       "              (layer_norm): LayerNormalization()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): EncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (feed_forward): FeedForwardLayer(\n",
       "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          )\n",
       "          (residual_connections): ModuleList(\n",
       "            (0): ResidualConnectionLayer(\n",
       "              (layer_norm): LayerNormalization()\n",
       "            )\n",
       "            (1): ResidualConnectionLayer(\n",
       "              (layer_norm): LayerNormalization()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNormalization()\n",
       "    )\n",
       "  )\n",
       "  (self_attn): MultiHeadAttention(\n",
       "    (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (feed_forward): FeedForwardLayer(\n",
       "    (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "virprobert_model.load_state_dict(torch.load(virprobert_model_file_path, map_location=nn_utils.get_device()))\n",
    "virprobert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33ce1972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset(df, filter_threshold, per_class_samples=0):\n",
    "    label_col = label_settings[\"label_col\"]\n",
    "    print(f\"Before filter df size = {df.shape}\")\n",
    "    label_count_map = dict(df[label_col].value_counts())\n",
    "    print(f\"Before filter # of unique values = {len(label_count_map)}\")\n",
    "    label_count_map = dict(filter(lambda x: x[1] >= filter_threshold, label_count_map.items()))\n",
    "    print(f\"After filter on min of samples # of unique values = {len(label_count_map)}\")\n",
    "    \n",
    "    selected_labels = list(label_count_map.keys())\n",
    "    df = df[df[label_col].isin(selected_labels)]\n",
    "    print(f\"After filter df size = {df.shape}\")\n",
    "    \n",
    "    df = pd.concat([df[df[label_col] == k][:min(v + 1, per_class_samples)] for k, v in label_count_map.items()])\n",
    "    print(f\"After filter on per_class_samples df size = {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# read datasets using config properties\n",
    "def read_dataset(input_dir, input_file_names, cols):\n",
    "    datasets = []\n",
    "    for input_file_name in input_file_names:\n",
    "        input_file_path = os.path.join(input_dir, input_file_name)\n",
    "        df = pd.read_csv(input_file_path, usecols=cols)\n",
    "        print(f\"input file: {input_file_path}, size = {df.shape}\")\n",
    "        datasets.append(df)\n",
    "\n",
    "    df = pd.concat(datasets)\n",
    "    print(f\"Size of input dataset = {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_token_dataset_loader(df, sequence_settings, label_col):\n",
    "    seq_col = sequence_settings[\"sequence_col\"]\n",
    "    batch_size = sequence_settings[\"batch_size\"]\n",
    "    max_seq_len = sequence_settings[\"max_sequence_length\"]\n",
    "    truncate = sequence_settings[\"truncate\"]\n",
    "    split_sequence = sequence_settings[\"split_sequence\"]\n",
    "\n",
    "    dataset = ProteinSequenceDataset(df, seq_col, label_col, truncate, max_seq_len)\n",
    "    collate_func = Padding(max_seq_len)       \n",
    "\n",
    "    return DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_func)\n",
    "\n",
    "def load_dataset(input_dir, input_file_names, sequence_settings, filter_threshold, per_class_samples):\n",
    "    df = read_dataset(input_dir, input_file_names, cols=[sequence_settings[\"sequence_col\"], label_settings[\"label_col\"]])\n",
    "    df = filter_dataset(df, filter_threshold, per_class_samples)\n",
    "    label_idx_map, idx_label_map = utils.get_label_vocabulary(list(df[label_settings[\"label_col\"]].unique()))\n",
    "    df.replace({label_settings[\"label_col\"]:label_idx_map}, inplace=True)\n",
    "    dataset_loader = get_token_dataset_loader(df, sequence_settings, label_settings[\"label_col\"])\n",
    "    print(df.head())\n",
    "    return df, dataset_loader, idx_label_map\n",
    "\n",
    "def compute_dataset_representations(model, dataset_loader):\n",
    "    model.eval()\n",
    "    seq_dfs = []\n",
    "    for _, record in enumerate(dataset_loader):\n",
    "        seq, label = record\n",
    "        seq_encoding = model.get_embedding(seq)\n",
    "        seq_df = pd.DataFrame(seq_encoding.squeeze().cpu().detach().numpy())\n",
    "        seq_df[\"label\"] = label.squeeze().cpu().detach().numpy()\n",
    "        seq_dfs.append(seq_df)\n",
    "    df = pd.concat(seq_dfs)\n",
    "    print(df.shape)\n",
    "    return df\n",
    "\n",
    "def print_dataset_loader(dataset_loader):\n",
    "    sequence, label = next(iter(dataset_loader))\n",
    "    print(sequence.shape)\n",
    "    print(sequence)\n",
    "    print(label.shape)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b537bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input file: /home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/few_shot_learning/../../../input/data/uniref90/20240131/uniref90_viridae_embl_hosts_pruned_metadata_species_vertebrates_w_seq_non_idv_lt_1_gte_0.05_prcnt_prevalence_seq_len_in_99prcntile.csv, size = (11116, 2)\n",
      "Size of input dataset = (11116, 2)\n",
      "Before filter df size = (11116, 2)\n",
      "Before filter # of unique values = 143\n",
      "After filter on min of samples # of unique values = 143\n",
      "After filter df size = (11116, 2)\n",
      "After filter on per_class_samples df size = (715, 2)\n",
      "      virus_host_name                                                seq\n",
      "2026               33  MKMFVLVGFVLFVVASATTTVNINVTTNGNHNVTSSNSNVLLQNRT...\n",
      "2027               33  MKTQLYILILYFLGVSSSQETTALLDPDRFCLQTDFSRILVFPKFR...\n",
      "2028               33  MKTLVSICFFITLFILTNSDPSCYDGLVENSRKNLDRPNSLAAYDL...\n",
      "2029               33  MGIHALNYIASNFETDDLVPTLFGACGVFAFLIIIGTVLFVCSGRM...\n",
      "2030               33  MNSTLLVISNPENQFTIDFILSGYINNTHYSIIVKDIKEESDGRFD...\n",
      "torch.Size([4, 2372])\n",
      "tensor([[22.,  7., 22.,  ...,  0.,  0.,  0.],\n",
      "        [13.,  5., 14.,  ...,  4., 17., 14.],\n",
      "        [13., 12., 11.,  ...,  0.,  0.,  0.],\n",
      "        [13.,  8.,  3.,  ...,  0.,  0.,  0.]], dtype=torch.float64)\n",
      "torch.Size([4])\n",
      "tensor([ 79,  77, 120,  88])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, dataset_loader, idx_label_map = load_dataset(input_dir, input_file_names, sequence_settings, filter_threshold=15, per_class_samples=5)\n",
    "print_dataset_loader(dataset_loader)\n",
    "len(idx_label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fffdf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "virprobert_rep_df = compute_dataset_representations(virprobert_model, dataset_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc366fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[label_settings[\"label_col\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f0f79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a26f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
