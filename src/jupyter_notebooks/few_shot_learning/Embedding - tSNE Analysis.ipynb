{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c13ffe",
   "metadata": {},
   "source": [
    "## TSNE Analysis of VirProBERT Few Shot Classifier\n",
    "### Dataset: EMBL mapping, Vertebrates, Non-IDV\n",
    "\n",
    "**Models**: VirProBERT, Few-Shot Classifier\n",
    "\n",
    "**Maximum Sequence Length**: 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b73dcb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/few_shot_learning',\n",
       " '/opt/conda/lib/python38.zip',\n",
       " '/opt/conda/lib/python3.8',\n",
       " '/opt/conda/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/home/blessyantony/.local/lib/python3.8/site-packages',\n",
       " '/opt/conda/lib/python3.8/site-packages',\n",
       " '/opt/conda/lib/python3.8/site-packages/IPython/extensions',\n",
       " '/home/blessyantony/.ipython',\n",
       " '/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/few_shot_learning/../../../../..',\n",
       " '/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/few_shot_learning/../../../..',\n",
       " '/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/few_shot_learning/../../..',\n",
       " '/home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/few_shot_learning/../..']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"..\", \"..\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"..\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\", \"..\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deda438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import utils, nn_utils, dataset_utils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from src.utils import utils, nn_utils\n",
    "from src.models.nlp.transformer import transformer\n",
    "from src.transfer_learning.fine_tuning import host_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abc115e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "virprobert_model_file_path = os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"output/raw/uniref90_embl_vertebrates_non_idv_t0.01_c5/20240611/host_multi/fine_tuning/host_prediction_fnn_2l_d1024_lr1e-4_itr0.pth\")\n",
    "virprobert_few_shot_classifier_model_file_path = os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"output/raw/uniref90_embl_vertebrates_non_idv/20240625/host_multi/few_shot_learning/fsl_tr_w3s5q10_te_w3s5q-1_e25b32_msl2048_split70-10-20_virprobert_itr4.pth\")\n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"input/data/uniref90/20240131\")\n",
    "input_file_names = [\"uniref90_viridae_embl_hosts_pruned_metadata_species_vertebrates_w_seq_non_idv_lt_1percent_prevalence.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74d69057",
   "metadata": {},
   "outputs": [],
   "source": [
    "virprobert_settings = {\n",
    "    \"mlm_encoder_settings\": {\n",
    "        \"embedding\": \"linear\",\n",
    "        \"n_heads\": 8,\n",
    "        \"depth\": 6,\n",
    "        \"input_dim\": 512, # input embedding dimension\n",
    "        \"hidden_dim\": 1024,\n",
    "        \"vocab_size\": 28\n",
    "    },\n",
    "    \"host_prediction_settings\": {\n",
    "        \"depth\": 2,\n",
    "        \"n_classes\": 5,\n",
    "        \"input_dim\": 512, # input embedding dimension\n",
    "        \"hidden_dim\": 1024\n",
    "    }\n",
    "}\n",
    "\n",
    "sequence_settings= {\n",
    "    \"id_col\": \"uniref90_id\",\n",
    "    \"sequence_col\": \"seq\",\n",
    "    \"max_sequence_length\": 2048, # 6630 # 1024 # 1115\n",
    "    \"truncate\": True,\n",
    "    \"split_sequence\": False,\n",
    "    \"pad_token_val\": 0,\n",
    "    \"feature_type\": \"token\",\n",
    "    \"batch_size\": 8,\n",
    "}\n",
    "\n",
    "label_settings= {\n",
    "    \"label_col\": \"virus_host_name\"\n",
    "}\n",
    "max_seq_len = sequence_settings[\"max_sequence_length\"]\n",
    "virprobert_settings[\"mlm_encoder_settings\"][\"max_seq_len\"] = max_seq_len\n",
    "virprobert_settings[\"host_prediction_settings\"][\"max_seq_len\"] = max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a08c9286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerEncoder(\n",
      "  (embedding): EmbeddingLayer(\n",
      "    (token_embedding): Embedding(28, 512, padding_idx=0)\n",
      "    (positional_embedding): PositionalEncoding()\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForwardLayer(\n",
      "          (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "          (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        )\n",
      "        (residual_connections): ModuleList(\n",
      "          (0): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "          (1): ResidualConnectionLayer(\n",
      "            (norm): NormalizationLayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): NormalizationLayer()\n",
      "  )\n",
      ")\n",
      "Number of parameters =  12618752\n",
      "HostPrediction(\n",
      "  (pre_trained_model): TransformerEncoder(\n",
      "    (embedding): EmbeddingLayer(\n",
      "      (token_embedding): Embedding(28, 512, padding_idx=0)\n",
      "      (positional_embedding): PositionalEncoding()\n",
      "    )\n",
      "    (encoder): Encoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): EncoderLayer(\n",
      "          (self_attn): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (W_O): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (W_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "            (W_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "          (residual_connections): ModuleList(\n",
      "            (0): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "            (1): ResidualConnectionLayer(\n",
      "              (norm): NormalizationLayer()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm): NormalizationLayer()\n",
      "    )\n",
      "  )\n",
      "  (linear_ip): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (linear_hidden): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (linear_hidden_n): ModuleList(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (linear_op): Linear(in_features=1024, out_features=5, bias=True)\n",
      ")\n",
      "Number of parameters =  16297989\n"
     ]
    }
   ],
   "source": [
    "mlm_encoder_model = transformer.get_transformer_encoder(virprobert_settings[\"mlm_encoder_settings\"])\n",
    "virprobert_settings[\"host_prediction_settings\"][\"pre_trained_model\"] = mlm_encoder_model\n",
    "pre_trained_model = host_prediction.get_host_prediction_model(virprobert_settings[\"host_prediction_settings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cf2a9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained_model.load_state_dict(torch.load(virprobert_model_file_path, map_location=nn_utils.get_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33ce1972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset(df, filter_threshold, per_class_samples=0):\n",
    "    label_col = label_settings[\"label_col\"]\n",
    "    print(f\"Before filter df size = {df.shape}\")\n",
    "    label_count_map = dict(df[label_col].value_counts())\n",
    "    print(f\"Before filter # of unique values = {len(label_count_map)}\")\n",
    "    label_count_map = dict(filter(lambda x: x[1] >= filter_threshold, label_count_map.items()))\n",
    "    print(f\"After filter on min of samples # of unique values = {len(label_count_map)}\")\n",
    "    \n",
    "    selected_labels = list(label_count_map.keys())\n",
    "    df = df[df[label_col].isin(selected_labels)]\n",
    "    print(f\"After filter df size = {df.shape}\")\n",
    "    \n",
    "    df = pd.concat([df[df[label_col] == k][:min(v + 1, per_class_samples)] for k, v in label_count_map.items()])\n",
    "    print(f\"After filter on per_class_samples df size = {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_dataset(input_dir, input_file_names, sequence_settings, filter_threshold, per_class_samples):\n",
    "    df = dataset_utils.read_dataset(input_dir, input_file_names, cols=[sequence_settings[\"sequence_col\"], label_settings[\"label_col\"]])\n",
    "    df = filter_dataset(df, filter_threshold, per_class_samples)\n",
    "    label_idx_map, idx_label_map = utils.get_label_vocabulary(list(df[label_settings[\"label_col\"]].unique()))\n",
    "    df.replace({label_settings[\"label_col\"]:label_idx_map}, inplace=True)\n",
    "    dataset_loader = dataset_utils.get_dataset_loader(df, sequence_settings, label_settings[\"label_col\"])\n",
    "    print(df.head())\n",
    "    return df, dataset_loader, idx_label_map\n",
    "\n",
    "def compute_dataset_representations(model, dataset_loader):\n",
    "    model.eval()\n",
    "    seq_dfs = []\n",
    "    for _, record in enumerate(dataset_loader):\n",
    "        seq, label = record\n",
    "        seq_encoding = model.get_embedding(seq)\n",
    "        seq_df = pd.DataFrame(seq_encoding.squeeze().cpu().detach().numpy())\n",
    "        seq_df[\"label\"] = label.squeeze().cpu().detach().numpy()\n",
    "        seq_dfs.append(seq_df)\n",
    "    df = pd.concat(seq_dfs)\n",
    "    print(df.shape)\n",
    "    return df\n",
    "\n",
    "def print_dataset_loader(dataset_loader):\n",
    "    sequence, label = next(iter(dataset_loader))\n",
    "    print(sequence.shape)\n",
    "    print(sequence)\n",
    "    print(label.shape)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b537bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input file: /home/blessyantony/dev/git/zoonosis/src/jupyter_notebooks/few_shot_learning/../../../input/data/uniref90/20240131/uniref90_viridae_embl_hosts_pruned_metadata_species_vertebrates_w_seq_non_idv_lt_1percent_prevalence.csv, size = (16074, 2)\n",
      "Size of input dataset = (16074, 2)\n",
      "Before filter df size = (16074, 2)\n",
      "Before filter # of unique values = 1299\n",
      "After filter on min of samples # of unique values = 192\n",
      "After filter df size = (12112, 2)\n",
      "After filter on per_class_samples df size = (960, 2)\n",
      "      virus_host_name                                                seq\n",
      "3201               42  MKMFVLVGFVLFVVASATTTVNINVTTNGNHNVTSSNSNVLLQNRT...\n",
      "3202               42  MKTQLYILILYFLGVSSSQETTALLDPDRFCLQTDFSRILVFPKFR...\n",
      "3203               42  MKTLVSICFFITLFILTNSDPSCYDGLVENSRKNLDRPNSLAAYDL...\n",
      "3204               42  MGIHALNYIASNFETDDLVPTLFGACGVFAFLIIIGTVLFVCSGRM...\n",
      "3205               42  MNSTLLVISNPENQFTIDFILSGYINNTHYSIIVKDIKEESDGRFD...\n",
      "torch.Size([8, 2048])\n",
      "tensor([[13.,  7.,  3.,  ...,  0.,  0.,  0.],\n",
      "        [13., 15., 12.,  ...,  0.,  0.,  0.],\n",
      "        [13., 15., 12.,  ...,  0.,  0.,  0.],\n",
      "        ...,\n",
      "        [13.,  3., 19.,  ...,  0.,  0.,  0.],\n",
      "        [13.,  8.,  1.,  ...,  0.,  0.,  0.],\n",
      "        [13.,  8.,  3.,  ...,  0.,  0.,  0.]], dtype=torch.float64)\n",
      "torch.Size([8])\n",
      "tensor([ 95, 189,  96, 170,  54,  66,  21, 173])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, dataset_loader, idx_label_map = load_dataset(input_dir, input_file_names, sequence_settings, filter_threshold=15, per_class_samples=5)\n",
    "print_dataset_loader(dataset_loader)\n",
    "len(idx_label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fffdf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "virprobert_rep_df = compute_dataset_representations(pre_trained_model, dataset_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc366fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[label_settings[\"label_col\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f0f79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a26f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
